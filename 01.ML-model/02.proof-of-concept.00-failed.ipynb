{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badfab16",
   "metadata": {},
   "source": [
    "# Reasoning\n",
    "\n",
    "## Why was this chosen?\n",
    "\n",
    "Unfortunately, with the AI *.ipynb* files losing working code too fast at the moment, it means copilot has a lot of old flawed examples to help with. It will take more than the hackathon time to get the right solution together using the model I wanted to use.\n",
    "\n",
    "That being said, there are at least models that can fit [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) should be a good model to train as the assistant (training in the future should be fine from copliot generated code in *01*, just needs work, mostly on prompts my side). We shall see in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16368e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install -q --upgrade trl --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install -q accelerate --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffda57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 µs, sys: 6 µs, total: 32 µs\n",
      "Wall time: 35.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hide Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c6083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MODEL_NAME = 'NousResearch/Llama-3.2-1B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29115e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6458b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592cabcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 07:44:52.412385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-13 07:44:52.425447: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-13 07:44:52.429425: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-13 07:44:52.440496: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-13 07:44:59.275499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 6.48 s, total: 17.7 s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load the Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6f83d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight -> cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.0.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.0.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.0.input_layernorm.weight -> cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.1.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.1.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.1.input_layernorm.weight -> cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.2.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.2.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.2.input_layernorm.weight -> cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.3.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.3.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.3.input_layernorm.weight -> cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.4.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.4.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.4.input_layernorm.weight -> cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.5.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.5.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.5.input_layernorm.weight -> cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.6.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.6.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.6.input_layernorm.weight -> cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.7.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.7.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.7.input_layernorm.weight -> cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.8.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.8.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.8.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.8.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.8.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.8.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.8.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.8.input_layernorm.weight -> cuda:0\n",
      "model.layers.8.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.9.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.9.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.9.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.9.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.9.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.9.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.9.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.9.input_layernorm.weight -> cuda:0\n",
      "model.layers.9.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.10.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.10.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.10.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.10.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.10.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.10.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.10.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.10.input_layernorm.weight -> cuda:0\n",
      "model.layers.10.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.11.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.11.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.11.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.11.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.11.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.11.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.11.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.11.input_layernorm.weight -> cuda:0\n",
      "model.layers.11.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.12.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.12.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.12.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.12.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.12.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.12.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.12.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.12.input_layernorm.weight -> cuda:0\n",
      "model.layers.12.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.13.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.13.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.13.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.13.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.13.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.13.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.13.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.13.input_layernorm.weight -> cuda:0\n",
      "model.layers.13.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.14.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.14.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.14.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.14.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.14.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.14.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.14.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.14.input_layernorm.weight -> cuda:0\n",
      "model.layers.14.post_attention_layernorm.weight -> cuda:0\n",
      "model.layers.15.self_attn.q_proj.weight -> cuda:0\n",
      "model.layers.15.self_attn.k_proj.weight -> cuda:0\n",
      "model.layers.15.self_attn.v_proj.weight -> cuda:0\n",
      "model.layers.15.self_attn.o_proj.weight -> cuda:0\n",
      "model.layers.15.mlp.gate_proj.weight -> cuda:0\n",
      "model.layers.15.mlp.up_proj.weight -> cuda:0\n",
      "model.layers.15.mlp.down_proj.weight -> cuda:0\n",
      "model.layers.15.input_layernorm.weight -> cuda:0\n",
      "model.layers.15.post_attention_layernorm.weight -> cuda:0\n",
      "model.norm.weight -> cuda:0\n",
      "CPU times: user 0 ns, sys: 5.53 ms, total: 5.53 ms\n",
      "Wall time: 68.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in model.named_parameters():\n",
    "    print(f\"{i[0]} -> {i[1].device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc30c6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 ms, sys: 37.4 ms, total: 39.6 ms\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_text = 'Im not having a great day, I did my exercises, and suggested theropy, but now Im bored. What can I try out that can engage my mind, and keep me entertained?'\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d564dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.2 s, sys: 1.13 s, total: 36.4 s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_ids = input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c86effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im not having a great day, I did my exercises, and suggested theropy, but now Im bored. What can I try out that can engage my mind, and keep me entertained? I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want to know what you guys think.\n",
      "I have a few ideas, but I want to know what you guys think. I have a few ideas, but I want\n",
      "CPU times: user 8.99 ms, sys: 11.6 ms, total: 20.5 ms\n",
      "Wall time: 973 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24cd59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
